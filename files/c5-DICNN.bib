@article{Daojunliang2020DICNN,
author = {Daojun Liang and Xiuping Wang and Xiaohui Ju and Feng Yang},
title = {Deeply Integrated Convolutional Neural Networks},
journal = {Journal of Computers},
volume = {31},
number = {1},
pages = {46-56},
keywords = {inference mechanisms, neural nets, random processes, multisample inference network, neural networks, multiinput multiprediction network architecture, MSIN, network shared parameters, generative adversarial network, multiple sample domains, data augmentation, random combination, generalisation performance},
doi = {https://doi.org/10.3966/199115992020023101004},
abstract = {The ensemble learning system based on neural network requires a large number of networks as the basic classifier, which makes the parameters and calculations of the system increase sharply. Integrating the neural network in depth can not only reduce the parameters and calculations of the network, but also improve the overall network performance. In this paper, a deeply integrated convolutional neural network (DICNN) was proposed, and several different integration methods were proposed for integrated learning of DICNN. The Mixup is used to train the DICNN because it uses multiple samples for training and it can be better combined with DICNN. A series of ablation experiments were done to prove that the training method of Mixup is equivalent to a regularization and data augmentation. Therefore, a different multi-sample training method as variations of the Mixup (Mixup-XL) can be used to train the DICNN.},
year = {2020}
}

